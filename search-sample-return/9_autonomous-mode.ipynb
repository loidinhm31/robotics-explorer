{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Autonomous Mode\n",
    "\n",
    "![alt text](images/rover-autonomous-example-1.gif)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "43511a90c76e3604"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Once you have settled on the appropriate analysis for the perception task you'll implement those within the Python scripts provided in the project repository. Here's an overview of how you'll implement autonomous navigation and mapping.\n",
    "\n",
    "## Navigating the Environment\n",
    "\n",
    "\n",
    "The main script you will use for autonomous navigation and mapping is called `drive_rover.py` (you can find it in the `code` folder in the [project repository](https://github.com/udacity/RoboND-Rover-Project). In this section, we'll step through this file more or less line by line so you know what's going on. You are free to modify `drive_rover.py`, but it's not required and should run as-is for the purposes of the project.\n",
    "\n",
    "At the top of the file you have a bunch of imports, but the two most relevant ones to you are these two:\n",
    "\n",
    "```python\n",
    "# Import functions for perception and decision making\n",
    "from perception import perception_step\n",
    "from decision import decision_step\n",
    "```\n",
    "\n",
    "\n",
    "`perception.py` and `decision.py` are also included in the project repository and are where you will modify and build out the perception and decision making steps of the process.\n",
    "\n",
    "These two scripts have already been populated with some starter code. In the case of `perception.py` it contains all the functions from the lesson and one empty function called `perception_step()`. Your job here is to populate `perception_step()` with the appropriate analyses and update the `Rover()` object accordingly.\n",
    "\n",
    "In the case of `decision.py`, the function `decision_step()` awaits your modification. It contains some example conditional statements that demonstrate how you might make decisions about adjusting throttle, brake and steering inputs. Until you update `Rover.nav_angles` the default decision is to have the rover accelerate straight forward up to maximum speed. Once you apply your analysis in `perception_step()` and update the `Rover.nav_angles` field, you'll see the rover is capable of a bit more complex navigation. It's up to you to build your decision treeâ€¦ your artificial intelligence to give the rover a brain!\n",
    "\n",
    "Supporting Functions\n",
    "After that comes one more import of note:\n",
    "\n",
    "```python\n",
    "from supporting_functions import update_rover, create_output_images\n",
    "```\n",
    "\n",
    "Have a look at supporting_functions.py to see what these functions are doing. In update_rover() your RoverState() object gets updated with each new batch of telemetry. The create_output_images() function is where your Rover.worldmap is compared with the ground truth map and gets converted, along with Rover.vision_image, into base64 strings to send back to the rover. You don't need to modify this code, but are welcome to do so if you would like to change what the display images look like, or what is displayed."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "aec09a51b5b76c06"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## SocketIO Server\n",
    "\n",
    "\n",
    "Next, you'll find some initialization of the SocketIO server that you don't need to worry about (though if you're curious you can learn more [here](https://python-socketio.readthedocs.io/en/latest/). After that, you're reading in the ground truth map of the environment for comparison with later\n",
    "\n",
    "```python\n",
    "# Read in ground truth map and create 3-channel green version for overplotting\n",
    "# NOTE: images are read in by default with the origin (0, 0) in the upper left\n",
    "# and y-axis increasing downward.\n",
    "ground_truth = mpimg.imread('../calibration_images/map_bw.png')\n",
    "# This next line creates arrays of zeros in the red and blue channels\n",
    "# and puts the map into the green channel.  This is why the underlying \n",
    "# map output looks green in the display image\n",
    "ground_truth_3d = np.dstack((ground_truth*0, ground_truth*255, ground_truth*0)).astype(np.float)\n",
    "```"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bfe493b5e28a34df"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## RoverState() and telemetry()\n",
    "\n",
    "\n",
    "Next you define and initialize your `RoverState()` class (as discussed here), which will allow you to keep track of telemetry values and results from your analysis."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "819306e8468eafec"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Next comes the definition of the `telemetry()` function. This function will be run every time the simulator sends a new batch of data (nominally 25 times per second). The first thing this function does is update the `Rover()` object with new telemetry values. After that, it calls the `perception_step()` and `decision_step()` functions to update analysis. Finally, it prepares the commands and images to be sent back to the rover and calls the `send_control()` function.\n",
    "\n",
    "```python\n",
    "# Define telemetry function for what to do with incoming data\n",
    "@sio.on('telemetry')\n",
    "def telemetry(sid, data):\n",
    "    if data:\n",
    "        global Rover\n",
    "\n",
    "        # Initialize / update Rover with current telemetry\n",
    "        Rover, image = update_rover(Rover, data)\n",
    "\n",
    "        if np.isfinite(Rover.vel):\n",
    "\n",
    "            # Execute the perception and decision steps to update the Rover's state\n",
    "            Rover = perception_step(Rover)\n",
    "            Rover = decision_step(Rover)\n",
    "\n",
    "            # Create output images to send to server\n",
    "            out_image_string1, out_image_string2 = create_output_images(Rover)\n",
    "\n",
    "            # The action step!  Send commands to the rover!\n",
    "            commands = (Rover.throttle, Rover.brake, Rover.steer)\n",
    "            send_control(commands, out_image_string1, out_image_string2)\n",
    "\n",
    "        # In case of invalid telemetry, send null commands\n",
    "        else:\n",
    "\n",
    "            # Send zeros for throttle, brake and steer and empty images\n",
    "            send_control((0, 0, 0), '', '')\n",
    "\n",
    "        # If you want to save camera images from autonomous driving specify a path\n",
    "        # Example: $ python drive_rover.py image_folder_path\n",
    "        # Conditional to save image frame if folder was specified\n",
    "        if args.image_folder != '':\n",
    "            timestamp = datetime.utcnow().strftime('%Y_%m_%d_%H_%M_%S_%f')[:-3]\n",
    "            image_filename = os.path.join(args.image_folder, timestamp)\n",
    "            image.save('{}.jpg'.format(image_filename))\n",
    "\n",
    "    else:\n",
    "        sio.emit('manual', data={}, skip_sid=True)\n",
    "```"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a6fb6078966edc5a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "You'll notice a section at the bottom of the telemetry function that allows for saving images from your autonomous navigation run if you like. The way to do that is to call drive_rover.py with an additional argument specifying the folder you want to save images to like this:\n",
    "\n",
    "```shell\n",
    "$ python drive_rover.py path_to_folder\n",
    "```"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e29d92d6a4484109"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Launch in Autonomous Mode!\n",
    "\n",
    "\n",
    "To get started with autonomous driving in the simulator, go ahead and run `drive_rover.py` by calling it in the following manner at the terminal prompt (and you should see similar output as displayed below):\n",
    "\n",
    "```shell\n",
    "python drive_rover.py\n",
    "NOT recording this run ...\n",
    "(1439) wsgi starting up on http://0.0.0.0:4567\n",
    "```\n",
    "\n",
    "You can also record images while in autonomous mode by providing a path to where you want to save images when you call `drive_rover.py` like this:\n",
    "```shell\n",
    "python drive_rover.py path_to_folder\n",
    "```\n",
    "\n",
    "Now launch the simulator and click on \"Autonomous Mode\". You should see the rover take off and start driving, while also publishing images to the screen. It doesn't drive very well yet and it's your job to teach it how to drive better!\n",
    "\n",
    "**Note: at any time in autonomous mode you can take over the controls using the arrow keys. This can help if you want to try out a particular series of maneuvers or get unstuck.**"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7509093552673792"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "83c487e98b2be623"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
